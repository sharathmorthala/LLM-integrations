ğŸ§  Local RAG System with FastAPI, LangChain & Ollama

This project is a fully local Retrieval-Augmented Generation (RAG) system built using FastAPI, LangChain, Chroma, and Ollama.
It allows you to ask questions about codebases and PDFs, and get grounded answers with source references, all running locally on your machine.

No cloud APIs. No data leaves your system.

ğŸš€ Features

âœ… Local LLM inference using Ollama

âœ… RAG pipeline with LangChain

âœ… Persistent vector storage using Chroma

âœ… Supports code files and PDF documents

âœ… Source-grounded answers with citations

âœ… FastAPI backend with clean modular structure

âœ… Works fully offline after setup

ğŸ—ï¸ Architecture Overview
User Question
     â”‚
     â–¼
FastAPI (/ai/ask)
     â”‚
     â–¼
LangChain RAG Pipeline
     â”‚
     â”œâ”€â–º Chroma Vector DB (retrieval)
     â”‚       â””â”€ Embeddings via Ollama (nomic-embed-text)
     â”‚
     â””â”€â–º Local LLM via Ollama (qwen2.5:3b)
              â”‚
              â–¼
        Grounded Answer + Sources

ğŸ§© Tech Stack
Component	Technology
Backend API	FastAPI
LLM Runtime	Ollama
Chat Model	qwen2.5:3b
Embeddings	nomic-embed-text
RAG Framework	LangChain
Vector Store	Chroma
Document Types	Code files, PDFs
Language	Python 3.11
ğŸ“ Project Structure
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI app entrypoint
â”‚   â”œâ”€â”€ config.py            # Central configuration
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ ai.py             # API routes (/ask, /reindex)
â”‚   â””â”€â”€ ai/
â”‚       â”œâ”€â”€ llm.py            # Ollama LLM wrapper
â”‚       â”œâ”€â”€ embeddings.py     # Ollama embeddings
â”‚       â”œâ”€â”€ vectorstore.py    # Chroma setup
â”‚       â”œâ”€â”€ prompts.py        # System & RAG prompts
â”‚       â””â”€â”€ rag.py            # RAG orchestration logic
â”‚
â”œâ”€â”€ knowledge/
â”‚   â”œâ”€â”€ code/                # Source code to index
â”‚   â””â”€â”€ pdfs/                # PDF documents to index
â”‚
â”œâ”€â”€ chroma_db/               # Persistent vector store (gitignored)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Prerequisites

Python 3.10+

Ollama installed and running

Git

ğŸ”§ Setup Instructions
1ï¸âƒ£ Clone the repository
git clone https://github.com/sharathmorthala/LLM-integrations.git
cd backend

2ï¸âƒ£ Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ§  Setup Ollama Models
Start Ollama
ollama serve

Pull required models
ollama pull qwen2.5:3b
ollama pull nomic-embed-text


Verify:

ollama list

â–¶ï¸ Run the Backend
uvicorn app.main:app --reload


Server will start at:

http://127.0.0.1:8000

ğŸ“š Index Knowledge (Required)

Before asking questions, index your documents:

curl -X POST http://127.0.0.1:8000/ai/reindex


This:

Loads code + PDFs

Splits into chunks

Generates embeddings

Stores them in Chroma

â“ Ask Questions

Example:

curl -X POST http://127.0.0.1:8000/ai/ask \
  -H "Content-Type: application/json" \
  -d '{"question":"What does the main service do? Provide file references."}'

Sample Response
{
  "question": "...",
  "answer": "...",
  "sources": [
    {
      "source": "knowledge/code/main_service.py",
      "snippet": "Main Service Entry Point..."
    }
  ],
  "retrieved_chunks": 4,
  "llm_model": "qwen2.5:3b",
  "embed_model": "nomic-embed-text"
}

â±ï¸ Performance Notes

Runs fully on CPU by default

Large responses may take time (local inference)

Streaming support planned for better UX

Context size and model choice strongly affect latency

ğŸ”’ Privacy & Security

100% local execution

No external API calls

No data sent to third parties

Ideal for private codebases and documents

ğŸ›£ï¸ Roadmap

 Streaming responses (SSE)

 Frontend chat UI

 Agent tools (code navigation, file search)

 Model switching (fast vs accurate)

 GPU acceleration support

 Evaluation & metrics

ğŸ¯ Why This Project Matters

This project demonstrates:

Real-world RAG architecture

Local LLM deployment

LangChain production patterns

Backend API design

Vector search fundamentals

Itâ€™s designed as a learning platform, portfolio project, and foundation for AI products.

ğŸ“Œ Author

Built by Sharath Chandra
Exploring local AI systems, RAG architectures, and applied LLM engineering.